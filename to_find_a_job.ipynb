{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "#CHANGE THIS WITH THE RIGHT PATH!\n",
    "chromedriver = \"/chromedriver/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# Create a driver called \"driver.\"\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a function such that \n",
    "# 1) it will automatically stop once there are no more entries (hence the while loop) \n",
    "# 2) it will return the list the jobs links \n",
    "def scrap_job_links(job):\n",
    "    pg_num = 0\n",
    "    jobs_links = []\n",
    "    search_term = job.replace(' ','%20')\n",
    "    \n",
    "    while True: \n",
    "        link = 'https://www.mycareersfuture.sg/search?search={}&page={}'.format(search_term,pg_num)\n",
    "        driver.get(link)\n",
    "        sleep(6)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        if soup.findAll('div',{'class':'card relative '}) != []:\n",
    "            jobs_links.extend(['https://www.mycareersfuture.sg'+job.findAll('a')[0].get('href') for job in soup.findAll('div',{'class':'card relative '})])\n",
    "            pg_num += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return jobs_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyst_url = scrap_job_links('data analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
